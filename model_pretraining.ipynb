{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_pretraining\n",
    "from bert import optimization\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "logger = tf.get_logger()\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained weights\n",
    "The pretrained weights will be taken from tfhub.dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_hub = \"https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-256_A-4/1\"\n",
    "output_dir = \"finetuned_weights/in_task_pretraining/\"\n",
    "max_seq_len = 256  # The pre-training task will typically contain 2 segments of text i.e. 2* 128\n",
    "max_predictions_per_seq = 40  # The MLM task will have at most these many masked tokens per example\n",
    "train_files = [\"datasets/in_task_pretraining/train/mlm_max_seq_len_256.tfrecord\"]\n",
    "dev_files = [\"datasets/in_task_pretraining/dev/mlm_max_seq_len_256.tfrecord\"]\n",
    "tf.gfile.MakeDirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT + MLM and NSP layers\n",
    "Take the pretrained BERT model and set it up for masked language modelling and next sentence prediction (MLM + NSP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masked_lm_prediction(sequence_output, masked_lm_positions):\n",
    "    \"\"\"This implementation follows the original from\n",
    "    https://github.com/google-research/bert/blob/master/run_pretraining.py#L240\n",
    "    But makes it work with the tfhub model\n",
    "    \"\"\"\n",
    "    # Reuse the embedding matrix in the output projection\n",
    "    # The word embedding table is the first trainable tensor that gets created\n",
    "    word_embeddings = tf.trainable_variables()[0]\n",
    "    print(\"Re-useing embedding matrix: \", word_embeddings)\n",
    "    assert \"word_embeddings\" in word_embeddings.name\n",
    "    \n",
    "    # Slice out the masked positions\n",
    "    input_tensor = run_pretraining.gather_indexes(sequence_output, masked_lm_positions)\n",
    "        \n",
    "    # Use the embedding matrix to make the prediction and add a bias term\n",
    "    output_bias = tf.get_variable(\"output_bias\", shape=[word_embeddings.shape.as_list()[0]], initializer=tf.zeros_initializer())\n",
    "    logits = tf.matmul(input_tensor, word_embeddings, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "   \n",
    "    return logits\n",
    "    \n",
    "    \n",
    "def get_next_sentence_prediction(pooled_output):\n",
    "    \"\"\"This implementation follows the original from\n",
    "    https://github.com/google-research/bert/blob/master/run_pretraining.py#L285\n",
    "    But makes it work with the tfhub model\n",
    "    \"\"\"\n",
    "\n",
    "    hidden_size = pooled_output.shape.as_list()[-1]\n",
    "    with tf.variable_scope(\"next_sentence_prediction\"):\n",
    "        A = tf.get_variable(\"weights\", [hidden_size, 2], initializer=tf.glorot_uniform_initializer()) \n",
    "        bias = tf.get_variable(\"bias\", [2], initializer=tf.zeros_initializer())\n",
    "        logits = tf.nn.xw_plus_b(pooled_output, A, bias)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "\n",
    "def create_model(is_predicting, inputs):\n",
    "    input_ids, input_mask, segment_ids, masked_lm_positions = inputs\n",
    "    bert_module = hub.Module(bert_model_hub, trainable=True)\n",
    "    bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n",
    "    bert_outputs = bert_module(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
    "    \n",
    "\n",
    "    # Use \"sequence_output\" for the MLM task and add a layer on top\n",
    "    masked_lm_logits = get_masked_lm_prediction(bert_outputs[\"sequence_output\"], masked_lm_positions)\n",
    "    # Use \"pooled_output\" for NSP task and add a layer on top\n",
    "    next_sentence_logits = get_next_sentence_prediction(bert_outputs[\"pooled_output\"])\n",
    "    \n",
    "    \n",
    "    return masked_lm_logits, next_sentence_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to wrap this model into a tensorflow estimator which automates the training loop for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_builder(learning_rate, num_train_steps, num_warmup_steps):\n",
    "    def model_fn(features, labels, mode, params):\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        \n",
    "        # Additional inputs for masked language modelling and next sentence prediction\n",
    "        masked_lm_positions = features[\"masked_lm_positions\"]\n",
    "        \n",
    "        # Flatten the label information to treat batch as 1 sequence\n",
    "        masked_lm_weights = tf.reshape(features[\"masked_lm_weights\"], [-1])\n",
    "        masked_lm_ids = tf.reshape(features[\"masked_lm_ids\"], [-1])\n",
    "        next_sentence_labels = tf.reshape(features[\"next_sentence_labels\"], [-1])\n",
    "\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "\n",
    "        # TRAIN and EVAL\n",
    "        if not is_predicting:\n",
    "            \n",
    "            # Model definition\n",
    "            inputs = [input_ids, input_mask, segment_ids, masked_lm_positions]\n",
    "            masked_lm_logits, next_sentence_logits = create_model(is_predicting, inputs)\n",
    "            masked_lm_predictions = tf.argmax(masked_lm_logits, axis=-1)\n",
    "            next_sentence_predictions = tf.argmax(next_sentence_logits, axis=-1)\n",
    "            \n",
    "            # Losses\n",
    "            masked_lm_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                                    masked_lm_ids, masked_lm_logits, from_logits=True)\n",
    "            masked_lm_loss = masked_lm_loss*masked_lm_weights\n",
    "            masked_lm_loss = tf.reduce_mean(masked_lm_loss)\n",
    "            \n",
    "            next_sentence_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                                    next_sentence_labels, next_sentence_logits, from_logits=True)\n",
    "            next_sentence_loss = tf.reduce_mean(next_sentence_loss)\n",
    "            \n",
    "            # The training loss is the sum of the masked language model and next sentence prediciton losses\n",
    "            loss = masked_lm_loss + next_sentence_loss \n",
    "            train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "            \n",
    "            \n",
    "            # Summaries\n",
    "            tf.summary.scalar(\"masked_lm_cross_entropy_loss\", masked_lm_loss)\n",
    "            tf.summary.scalar(\"next_sentence_cross_entropy_loss\", next_sentence_loss)\n",
    "            masked_lm_accuracy, accuracy_op_0 = tf.metrics.accuracy(next_sentence_labels, next_sentence_predictions)\n",
    "            next_sentence_accuracy, accuracy_op_1 = tf.metrics.accuracy(next_sentence_labels, next_sentence_predictions)\n",
    "            \n",
    "            \n",
    "            with tf.control_dependencies([accuracy_op_0, accuracy_op_1]):\n",
    "                tf.summary.scalar(\"masked_lm_accuracy\", masked_lm_accuracy)\n",
    "                tf.summary.scalar(\"next_sentence_accuracy\", next_sentence_accuracy)\n",
    "                \n",
    "            # Extract the learning rate from the graph for logging\n",
    "            for o in tf.get_default_graph().get_operations():\n",
    "                if \"PolynomialDecay\" == o.name:\n",
    "                    print(o.name)\n",
    "                    lr = o.values()[0]                    \n",
    "            tf.summary.scalar(\"learning_rate\", lr)\n",
    "            \n",
    "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "            else:\n",
    "                # Calculate evaluation metrics. \n",
    "                eval_metrics = {}\n",
    "                eval_metrics[\"masked_lm_accuracy\"] = tf.metrics.accuracy(next_sentence_labels, next_sentence_predictions)\n",
    "                eval_metrics[\"next_sentence_accuracy\"] = tf.metrics.accuracy(next_sentence_labels, next_sentence_predictions)\n",
    "                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics)\n",
    "        else:\n",
    "            inputs = [input_ids, input_mask, segment_ids, masked_lm_positions]\n",
    "            masked_lm_logits, next_sentence_logits = create_model(is_predicting, inputs)\n",
    "            predictions = {'masked_lm_logts': masked_lm_logits, 'masked_lm_ids' : masked_lm_ids}\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # Return the actual model function in the closure\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "batch_size = 64\n",
    "learning_rate = 5e-5\n",
    "num_train_steps = 800000//batch_size # there are about 800k examples in the dataset\n",
    "num_warmup_steps = 0\n",
    "\n",
    "# Specify output directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(model_dir=output_dir, save_summary_steps=10,\n",
    "                                    save_checkpoints_steps=500, keep_checkpoint_max=2)\n",
    "\n",
    "model_fn = model_fn_builder( learning_rate=learning_rate, num_train_steps=num_train_steps,\n",
    "                            num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config, params={\"batch_size\": batch_size})\n",
    "\n",
    "# Load the in-task pretraining data\n",
    "train_input_fn = bert.run_pretraining.input_fn_builder(train_files, max_seq_len,\n",
    "                                                       max_predictions_per_seq, is_training=True)\n",
    "\n",
    "print(f\"Training for {num_train_steps} steps\")\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
