{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Foods in-task pre-training dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-task pre-training can help boost downstream fine-tuning performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import tokenization, optimization, create_pretraining_data\n",
    "logger = tf.get_logger()\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset settings \n",
    "In task Pre-training will re-use the Wikipedia pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_hub = \"https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-256_A-4/1\"\n",
    "max_seq_len = 256  # The pre-training task will typically contain 2 segments of text i.e. 2* 128\n",
    "max_predictions_per_seq = 40  # The MLM task will have at most these many masked tokens per example\n",
    "destination_folder = \"datasets/in_task_pretraining/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the raw dataset\n",
    "The train/dev/test splits were previously create in the create_finetuning_dataset notebook. The same split is used here, for consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/Reviews_train.csv\")\n",
    "train_text = data[\"Text\"]\n",
    "data = pd.read_csv(\"datasets/Reviews_dev.csv\")\n",
    "dev_text = data[\"Text\"]\n",
    "data = pd.read_csv(\"datasets/Reviews_test.csv\")\n",
    "test_text = data[\"Text\"]\n",
    "print(f\"Train dataset length is {len(train_text)} samples\")\n",
    "print(f\"Dev dataset length is {len(dev_text)} samples\")\n",
    "print(f\"Test dataset length is {len(test_text)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Preprocessing\n",
    "Bert pretraining data should contain one sentence per line and a blank line between different documents (i.e. Reviews). This section processes the raw csv data into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_raw_text_files(text, destination_folder, split):\n",
    "    output_dir = os.path.join(destination_folder, split)\n",
    "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
    "    output_file = None\n",
    "    \n",
    "    # Loop therough all the reviews\n",
    "    for i, t in enumerate(text):\n",
    "        \n",
    "        # Each file should contain no more than 50k reviews\n",
    "        if i%50000 == 0:\n",
    "            if output_file is not None: output_file.close()\n",
    "            output_file = open(os.path.join(output_dir, f\"file_{i//50000}\"), \"w\")\n",
    "        \n",
    "        # Write each sentence in the review on a separate line\n",
    "        for sentence in nltk.tokenize.sent_tokenize(t):\n",
    "            print(sentence, file=output_file)\n",
    "        \n",
    "        # Use a blank line between reviews\n",
    "        print(file=output_file)\n",
    "    \n",
    "    # Make sure file is closed\n",
    "    if not output_file.closed: output_file.close()\n",
    "    return\n",
    "\n",
    "write_raw_text_files(train_text, destination_folder, \"train\")\n",
    "write_raw_text_files(dev_text, destination_folder, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(bert_model_hub)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]])      \n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "    \n",
    "# Example of tokenization\n",
    "print(\"\\nEXAMPLE:\")\n",
    "example=\\\n",
    "\"\"\"\n",
    "I bought this after doing some research.\n",
    "This food is very good for your cat.\n",
    "My cats coat is all smooth now. <\\ br> He is more active.\n",
    "This is what cats we designed to eat.\n",
    "\"\"\"\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pretraining data\n",
    "The bert pacakge contains methods for creating pretraining data. The procesure used here is similar to https://github.com/google-research/bert/blob/master/create_pretraining_data.py, except that we use the checkpoint tokenizer information, rather than creating a new dictionary.\n",
    "Note that this step is quite slow and expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretraining_split(split):\n",
    "    input_pattern = os.path.join(destination_folder, split) + \"/file**\"\n",
    "    input_files = glob.glob(input_pattern)\n",
    "    output_file = os.path.join(destination_folder, split, f\"mlm_max_seq_len_{max_seq_len}.tfrecord\")\n",
    "    print(\"Using input files: \", input_files)\n",
    "    print(\"Using output file: \", output_file)\n",
    "    \n",
    "    rng = random.Random(0)\n",
    "    instances = create_pretraining_data.create_training_instances(\n",
    "        input_files, tokenizer, max_seq_len, dupe_factor=1,\n",
    "        short_seq_prob=0.02, masked_lm_prob=0.15, max_predictions_per_seq=max_predictions_per_seq, rng=rng)\n",
    "    print(\"Done creating instances\")\n",
    "    create_pretraining_data.write_instance_to_example_files(instances, tokenizer, max_seq_len,\n",
    "                                    max_predictions_per_seq, [output_file])\n",
    "    print(\"Done writting examples to file.\")\n",
    "    return\n",
    "\n",
    "create_pretraining_split(\"dev\")\n",
    "create_pretraining_split(\"train\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
