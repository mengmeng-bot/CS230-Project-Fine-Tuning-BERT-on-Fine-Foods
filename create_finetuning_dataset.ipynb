{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Foods dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "from bert import run_classifier, tokenization, optimization\n",
    "logger = tf.get_logger()\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset settings \n",
    "The featurization will depend on the vocab.txt of the bert model. For now, a smaller\n",
    "4 layer model is used, with a hidden size of 256 and 4 attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_hub = \"https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-256_A-4/1\"\n",
    "max_seq_len = 128  # this is relatively small and helps keep the compute cost down\n",
    "label_list = [1, 2, 3, 4, 5]  # reviews awards scores between 1 and 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset splits\n",
    "This is a relatively large dataset with ~600k examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/Reviews.csv\")\n",
    "all_text = data[\"Text\"]\n",
    "all_scores = data[\"Score\"]\n",
    "print(f\"Total dataset length is {len(all_text)} samples\")\n",
    "\n",
    "n_test = 20000\n",
    "n_dev  = 40000\n",
    "train_text, test_text, train_scores, test_scores = train_test_split(all_text, all_scores, test_size=n_test, shuffle=True)\n",
    "train_text, dev_text, train_scores, dev_scores = train_test_split(train_text, train_scores, test_size=n_dev, shuffle=True)\n",
    "print(f\"Train dataset has length {len(train_text)} samples\")\n",
    "print(f\"Dev dataset has length    {len(dev_text)} samples\")\n",
    "print(f\"Test dataset has length   {len(test_text)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset splits to disk\n",
    "This ensures the same dataset split is used when creating in-task fine-tuning data and in-task pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_split(text, scores, split_name):\n",
    "    data = {\"Text\": text, \"Score\": scores}\n",
    "    df = pd.concat(data, axis=1)\n",
    "    df.to_csv(f\"datasets/Reviews_{split_name}.csv\")\n",
    "    return\n",
    "\n",
    "save_split(train_text, train_scores, \"train\")\n",
    "save_split(test_text, test_scores, \"test\")\n",
    "save_split(dev_text, dev_scores, \"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset splits from disk\n",
    "If splits have already been saved to disk, they can be loaded here instead of created above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split_name):\n",
    "    df = pd.read_csv(f\"datasets/Reviews_{split_name}.csv\")\n",
    "    text = df[\"Text\"]\n",
    "    scores = df[\"Score\"]\n",
    "    return text, scores\n",
    "\n",
    "train_text, train_scores = load_split(\"train\")\n",
    "test_text, test_scores = load_split(\"test\")\n",
    "dev_text, dev_scores = load_split(\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of score distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title(\"Histogram of scores\")\n",
    "import numpy as np\n",
    "train_scores.hist(bins=np.linspace(0.5, 5, 10) + 0.25, align=\"mid\")\n",
    "plt.ylabel(\"No. examples\")\n",
    "plt.xlabel(\"Score (i.e. stars)\")\n",
    "plt.show()\n",
    "#plt.savefig(\"histogram_of_scores.png\", dpi=200, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the tokenizer\n",
    "The tokenizer does a few things (this is also included in the Python library):\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(bert_model_hub)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"], tokenization_info[\"do_lower_case\"]])      \n",
    "    tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "    \n",
    "# Example of tokenization\n",
    "print(\"\\nEXAMPLE:\")\n",
    "print(tokenizer.tokenize(\"Fine-tuning BERT on fine foods, with tricks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Preprocessing\n",
    "This proprocessing code is taken from the offical bert repo example: https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
    "\n",
    "We need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "For the baseline we only used `text_a` as the first 128 tokens of the text we wanted to classify, but for the final model we will use the  first 64 and the last 64 tokens of the review. The start and end tokens of reviews tend to contains the most information.\n",
    "\n",
    "- `text_a` is the first and last 64 tokens of the text that we want to classify, \n",
    "- `text_b` None\n",
    "- `label` is the label for our example, i.e. True, False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(text, scores):    \n",
    "    examples = []\n",
    "    for t, s in zip(text, scores):\n",
    "        t = t.replace(\"<br />\", \"\")   # custom data clean-up\n",
    "        tokens = tokenizer.tokenize(t)\n",
    "        \n",
    "        # If the entire review fits, then there is no need to\n",
    "        # use text b\n",
    "        if len(tokens) < 128:\n",
    "            text_a = t\n",
    "            text_b = None\n",
    "            \n",
    "        # If review is long, split into the first 64 and last 64 tokens\n",
    "        # only append complete sentences\\\n",
    "        else:\n",
    "            target_length = max_seq_len//2\n",
    "            text_a = \"\"\n",
    "            for sentence in nltk.tokenize.sent_tokenize(t):\n",
    "                if len(tokenizer.tokenize(text_a)) < target_length:\n",
    "                    text_a += sentence + \" \"\n",
    "                else:\n",
    "                    break\n",
    "            text_a = text_a.strip()  # remove trailing whitespace\n",
    "                \n",
    "            text_b = \"\"\n",
    "            for sentence in reversed(nltk.tokenize.sent_tokenize(t)):\n",
    "                if len(tokenizer.tokenize(sentence + \" \" + text_b)) <= target_length:\n",
    "                    text_b = sentence + \" \" + text_b\n",
    "                else:\n",
    "                    break\n",
    "            text_b = text_b.strip()  # remove trailing whitespace\n",
    "            text_a += \" \" + text_b\n",
    "            text_a = text_a.replace(\"  \", \" \")\n",
    "        \n",
    "        example = bert.run_classifier.InputExample(guid=None, text_a = text_a, text_b = None, label = s)\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "train_examples = create_examples(train_text, train_scores)\n",
    "dev_examples = create_examples(dev_text, dev_scores)\n",
    "test_examples = create_examples(test_text, test_scores)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write datasets to disk\n",
    "This fine foods dataset is relatively large, so writing it to disk may take around ~ 20 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = bert.run_classifier.file_based_convert_examples_to_features\n",
    "writer(train_examples, label_list, max_seq_len, tokenizer, \"datasets/training2\")\n",
    "writer(dev_examples, label_list, max_seq_len, tokenizer, \"datasets/dev2\")\n",
    "writer(test_examples, label_list, max_seq_len, tokenizer, \"datasets/test2\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
